#ABDUL RAFAY / 2022F-BSE-197 / LAB 07 / OPEN-ENDED LAB / STOCK PRICE PREDICTION:
print("ABDUL RAFAY / 2022F-BSE-197 / LAB 07 / OPEN-ENDED LAB / STOCK PRICE PREDICTION:\n")
#Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from tensorflow.keras.optimizers import SGD, Adam

# Download Apple stock data (2010-2023)
stock_symbol = "AAPL"
data = yf.download(stock_symbol, start="2010-01-01", end="2023-12-31")
data = data[['Close']]  # Use only 'Close' price
data.head()

#2. Data Preprocessing
#Normalize Data (0 to 1)
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)
#Create Time-Series Dataset
def create_dataset(data, time_step=60):
    X, y = [], []
    for i in range(time_step, len(data)):
        X.append(data[i-time_step:i, 0])
        y.append(data[i, 0])
    return np.array(X), np.array(y)

time_step = 60  # Use 60 days to predict next day
X, y = create_dataset(scaled_data, time_step)
#Train-Test Split (80-20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)
#Reshape for LSTM (Samples, Time Steps, Features)
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

#3. Model Implementations
#(1) Linear Regression (Baseline)
# Flatten data for Linear Regression
X_train_lr = X_train.reshape(X_train.shape[0], -1)
X_test_lr = X_test.reshape(X_test.shape[0], -1)
# Train Linear Regression
lr_model = LinearRegression()
lr_model.fit(X_train_lr, y_train)
lr_pred = lr_model.predict(X_test_lr)
# Evaluate
lr_rmse = np.sqrt(mean_squared_error(y_test, lr_pred))
print(f"Linear Regression RMSE: {lr_rmse}")

#(2) Single-Layer Perceptron (SLP) with SGD Optimizer
slp_model = Sequential([
    Dense(1, input_shape=(X_train_lr.shape[1],), activation='linear')  # Linear activation
])
slp_model.compile(optimizer=SGD(learning_rate=0.01), loss='mean_squared_error')
slp_model.fit(X_train_lr, y_train, epochs=50, batch_size=32, verbose=0)
slp_pred = slp_model.predict(X_test_lr)
# Evaluate
slp_rmse = np.sqrt(mean_squared_error(y_test, slp_pred))
print(f"SLP RMSE: {slp_rmse}")

#(3) Multi-Layer Neural Network (Keras)
mlp_model = Sequential([
    Dense(64, input_shape=(X_train_lr.shape[1],), activation='relu'),  # ReLU activation
    Dense(32, activation='relu'),
    Dense(1, activation='linear')  # Linear for regression
])
mlp_model.compile(optimizer=Adam(), loss='mean_squared_error')
mlp_model.fit(X_train_lr, y_train, epochs=50, batch_size=32, verbose=0)
mlp_pred = mlp_model.predict(X_test_lr)
# Evaluate
mlp_rmse = np.sqrt(mean_squared_error(y_test, mlp_pred))
print(f"MLP RMSE: {mlp_rmse}")

#(4) LSTM (Deep Learning)
lstm_model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)),
    Dropout(0.2),
    LSTM(50, return_sequences=False),
    Dropout(0.2),
    Dense(1, activation='linear')  # Linear for regression
])
lstm_model.compile(optimizer=Adam(), loss='mean_squared_error')
lstm_model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)
lstm_pred = lstm_model.predict(X_test)
# Evaluate
lstm_rmse = np.sqrt(mean_squared_error(y_test, lstm_pred))
print(f"LSTM RMSE: {lstm_rmse}")

#4. Prediction & Visualization
#Inverse Scaling (Back to Original Prices)
y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))
lstm_pred_actual = scaler.inverse_transform(lstm_pred)
#Plot Predictions vs Actual
plt.figure(figsize=(12, 6))
plt.plot(y_test_actual, label='Actual Price')
plt.plot(lstm_pred_actual, label='LSTM Predicted Price')
plt.title(f"{stock_symbol} Stock Price Prediction")
plt.xlabel('Time')
plt.ylabel('Price ($)')
plt.legend()
plt.show()
***************************************************************************************************************************

#ABDUL RAFAY / 2022F-BSE-197 / LAB 07 / OPEN-ENDED LAB / MEDICAL DIAGNOSIS (DISEASE DETECTION):
print("ABDUL RAFAY / 2022F-BSE-197 / LAB 07 / OPEN-ENDED LAB / MEDICAL DIAGNOSIS (DISEASE DETECTION):\n")
#Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.activations import sigmoid, relu, softmax

#Load Dataset
# Download dataset (or load from local)
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv"
df = pd.read_csv(url, header=None)
df.columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 
              'Insulin', 'BMI', 'DiabetesPedigree', 'Age', 'Outcome']
df.head()
#Exploratory Data Analysis
print(df.info())
print(df.describe())
df.hist(figsize=(12, 10))
plt.show()

#2. Data Preprocessing
#Handle Missing Values (if any)
# Replace zeros with mean (except 'Pregnancies' and 'Outcome')
cols_to_fix = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
for col in cols_to_fix:
    df[col] = df[col].replace(0, df[col].mean())
#Split Features & Target
X = df.drop('Outcome', axis=1)
y = df['Outcome']
#Normalize Data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
#Train-Test Split (80-20)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

#3. Model Implementations
#(1) Linear Regression (Auxiliary Prediction)
# Note: Linear Regression for probability prediction (not classification)
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
lr_pred_prob = lr_model.predict(X_test)
# Convert probabilities to binary predictions (0 or 1)
lr_pred = (lr_pred_prob > 0.5).astype(int)
# Evaluate
print("Linear Regression Accuracy:", accuracy_score(y_test, lr_pred))
print(confusion_matrix(y_test, lr_pred))
print(classification_report(y_test, lr_pred))

#(2) Single-Layer Perceptron (SLP) with Adam Optimizer
slp_model = Sequential([
    Dense(1, input_shape=(X_train.shape[1],), activation='sigmoid')  # Binary classification
])
slp_model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])
slp_model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)
# Evaluate
slp_pred = (slp_model.predict(X_test) > 0.5).astype(int)
print("SLP Accuracy:", accuracy_score(y_test, slp_pred))
print(confusion_matrix(y_test, slp_pred))

#(3) Multi-Layer Neural Network (Keras)
mlp_model = Sequential([
    Dense(64, input_shape=(X_train.shape[1],), activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Sigmoid for binary classification
])
mlp_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
mlp_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)
# Evaluate
mlp_pred = (mlp_model.predict(X_test) > 0.5).astype(int)
print("MLP Accuracy:", accuracy_score(y_test, mlp_pred))
print(classification_report(y_test, mlp_pred))

#(4) Deep Neural Network (DNN) with Keras
dnn_model = Sequential([
    Dense(128, input_shape=(X_train.shape[1],), activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])
dnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
history = dnn_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, verbose=1)
# Evaluate
dnn_pred = (dnn_model.predict(X_test) > 0.5).astype(int)
print("DNN Accuracy:", accuracy_score(y_test, dnn_pred))
print(confusion_matrix(y_test, dnn_pred))

#4. Model Comparison & Visualization
#Accuracy Comparison
models = ['Linear Regression', 'SLP', 'MLP', 'DNN']
accuracies = [
    accuracy_score(y_test, lr_pred),
    accuracy_score(y_test, slp_pred),
    accuracy_score(y_test, mlp_pred),
    accuracy_score(y_test, dnn_pred)
]
plt.figure(figsize=(10, 6))
plt.bar(models, accuracies, color=['blue', 'orange', 'green', 'red'])
plt.title("Model Accuracy Comparison")
plt.ylabel("Accuracy")
plt.ylim(0.6, 0.9)
plt.show()
#Training History Plot (DNN)
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Over Epochs')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Over Epochs')
plt.legend()
plt.show()

#5. Prediction on New Data
#Example Prediction
# Sample patient data (scaled)
new_patient = np.array([[6, 148, 72, 35, 0, 33.6, 0.627, 50]])  # From dataset
new_patient_scaled = scaler.transform(new_patient)
# Predict using DNN
prediction = dnn_model.predict(new_patient_scaled)
print("Probability of Diabetes:", prediction[0][0])
print("Diagnosis:", "Diabetic" if prediction > 0.5 else "Not Diabetic")